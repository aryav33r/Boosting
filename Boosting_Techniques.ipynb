{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1. What is Boosting in Machine Learning?\n",
        "Boosting is an ensemble method that combines multiple weak learners (models performing slightly better than random chance) into a strong learner. It works sequentially - each new model focuses on correcting errors made by previous models. Unlike bagging where models are independent, boosting creates models that complement each other. Popular algorithms include AdaBoost, Gradient Boosting, XGBoost, and CatBoost. The key principle is that subsequent learners focus more on difficult cases that earlier models misclassified.\n",
        "\n",
        "### 2. How does Boosting differ from Bagging?\n",
        "**Boosting:**\n",
        "- Sequential training of models\n",
        "- Models learn from previous errors\n",
        "- Weighted combination of models\n",
        "- Focuses on reducing bias\n",
        "- Examples: AdaBoost, Gradient Boosting\n",
        "\n",
        "**Bagging:**\n",
        "- Parallel training of independent models\n",
        "- Models trained on bootstrap samples\n",
        "- Simple averaging of predictions\n",
        "- Focuses on reducing variance\n",
        "- Example: Random Forest\n",
        "\n",
        "Key difference: Boosting adaptively changes the distribution of training data based on performance of previous models, while bagging uses random sampling with replacement.\n",
        "\n",
        "### 3. What is the key idea behind AdaBoost?\n",
        "AdaBoost (Adaptive Boosting) works by:\n",
        "1. Initially giving equal weight to all training instances\n",
        "2. Training a weak learner (typically a decision stump - 1-level tree)\n",
        "3. Increasing weights of misclassified instances\n",
        "4. Training subsequent models that focus more on difficult cases\n",
        "5. Combining all weak learners through weighted majority vote\n",
        "\n",
        "The weights depend on each learner's accuracy - more accurate learners get higher voting power. This adaptive reweighting is the algorithm's core innovation.\n",
        "\n",
        "### 4. Explain the working of AdaBoost with an example.\n",
        "**Example with 3 iterations on binary classification:**\n",
        "\n",
        "1. **First Model:**\n",
        "   - All points have equal weight\n",
        "   - Model 1 correctly classifies 80% (α₁=0.69)\n",
        "   - Misclassified points get higher weights\n",
        "\n",
        "2. **Second Model:**\n",
        "   - Focuses more on previously misclassified points\n",
        "   - Achieves 70% accuracy on weighted data (α₂=0.42)\n",
        "   - Updates weights again\n",
        "\n",
        "3. **Third Model:**\n",
        "   - Focuses on remaining hard cases\n",
        "   - Achieves 65% accuracy (α₃=0.27)\n",
        "\n",
        "**Final Prediction:**\n",
        "Weighted sum: α₁M₁ + α₂M₂ + α₃M₃ where α are based on each model's accuracy. The example shows how AdaBoost gives more influence to better-performing models.\n",
        "\n",
        "### 5. What is Gradient Boosting, and how is it different from AdaBoost?\n",
        "**Gradient Boosting:**\n",
        "- Builds models sequentially to minimize a loss function\n",
        "- Each new model predicts the residuals (errors) of previous ensemble\n",
        "- Uses gradient descent to optimize arbitrary differentiable loss functions\n",
        "- Typically uses deeper trees than AdaBoost\n",
        "\n",
        "**Key Differences:**\n",
        "- AdaBoost adjusts instance weights, GB fits to residuals\n",
        "- AdaBoost uses exponential loss, GB can use various losses (MSE, log-loss)\n",
        "- AdaBoost uses weighted vote, GB uses additive model\n",
        "- GB is generally more flexible and powerful\n",
        "\n",
        "### 6. What is the loss function in Gradient Boosting?\n",
        "Common loss functions include:\n",
        "\n",
        "**For Regression:**\n",
        "- Mean Squared Error (MSE): ½(y-ŷ)²\n",
        "- Mean Absolute Error (MAE): |y-ŷ|\n",
        "- Huber Loss: Combination of MSE and MAE\n",
        "\n",
        "**For Classification:**\n",
        "- Logistic Loss: log(1+exp(-2yŷ)) for y∈{-1,1}\n",
        "- Exponential Loss: exp(-yŷ)\n",
        "- Multinomial Deviance for multi-class\n",
        "\n",
        "The choice affects the algorithm's robustness and performance characteristics.\n",
        "\n",
        "### 7. How does XGBoost improve over traditional Gradient Boosting?\n",
        "XGBoost (Extreme Gradient Boosting) introduces:\n",
        "1. **Regularization:** L1 (Lasso) and L2 (Ridge) terms control overfitting\n",
        "2. **Parallel Processing:** Faster tree construction\n",
        "3. **Tree Pruning:** Grows trees depth-first then prunes backward\n",
        "4. **Handling Missing Values:** Learns default directions\n",
        "5. **Built-in Cross-Validation**\n",
        "6. **Hardware Optimization:** Cache-aware access, out-of-core computation\n",
        "7. **Sparsity Awareness:** Efficient handling of sparse data\n",
        "8. **Weighted Quantile Sketch:** For approximate tree learning\n",
        "\n",
        "These make XGBoost faster, more accurate, and more robust than traditional GB.\n",
        "\n",
        "### 8. What is the difference between XGBoost and CatBoost?\n",
        "**XGBoost:**\n",
        "- Requires manual categorical feature encoding\n",
        "- More hyperparameters to tune\n",
        "- Uses gradient-based tree splitting\n",
        "- Popular for general tabular data\n",
        "\n",
        "**CatBoost:**\n",
        "- Automatic categorical feature handling\n",
        "- Ordered boosting prevents target leakage\n",
        "- Symmetric trees for faster prediction\n",
        "- Better with high-cardinality categoricals\n",
        "- Built-in handling of missing values\n",
        "\n",
        "CatBoost often performs better with categorical data but may be slower than XGBoost on numerical data.\n",
        "\n",
        "### 9. What are some real-world applications of Boosting techniques?\n",
        "1. **Finance:** Credit scoring, fraud detection\n",
        "2. **Healthcare:** Disease diagnosis, patient risk stratification\n",
        "3. **Marketing:** Customer churn prediction, recommendation systems\n",
        "4. **Computer Vision:** Object detection, image classification\n",
        "5. **Search Engines:** Ranking algorithms\n",
        "6. **Manufacturing:** Predictive maintenance\n",
        "7. **Insurance:** Claims prediction\n",
        "8. **Cybersecurity:** Anomaly detection\n",
        "\n",
        "Boosting excels in problems requiring high predictive accuracy with structured data.\n",
        "\n",
        "### 10. How does regularization help in XGBoost?\n",
        "XGBoost's regularization:\n",
        "1. **L1 (alpha):** Encourages sparsity by shrinking less important features to zero\n",
        "2. **L2 (lambda):** Prevents large weights by penalizing squared magnitudes\n",
        "3. **Gamma:** Minimum loss reduction required for further splits\n",
        "4. **Max Depth:** Limits tree complexity\n",
        "5. **Subsample/Colsample:** Randomly selects subsets of data/features\n",
        "\n",
        "These controls prevent overfitting and improve generalization to unseen data.\n",
        "\n",
        "### 11. What are some hyperparameters to tune in Gradient Boosting models?\n",
        "Key hyperparameters:\n",
        "\n",
        "1. **n_estimators:** Number of boosting stages\n",
        "2. **learning_rate:** Shrinkage factor for contributions\n",
        "3. **max_depth:** Maximum tree depth\n",
        "4. **min_samples_split:** Minimum samples required to split\n",
        "5. **subsample:** Fraction of samples used per tree\n",
        "6. **max_features:** Number of features considered per split\n",
        "7. **loss:** Loss function to optimize\n",
        "8. **alpha:** For quantile regression/MAE\n",
        "\n",
        "Tuning these significantly impacts model performance.\n",
        "\n",
        "### 12. What is the concept of Feature Importance in Boosting?\n",
        "Feature importance measures:\n",
        "1. **Frequency:** How often a feature is used in splits\n",
        "2. **Gain:** Average reduction in loss a feature brings\n",
        "3. **Cover:** Number of samples affected by splits\n",
        "4. **Weight:** For tree ensembles, number of splits using the feature\n",
        "\n",
        "These help identify which features most influence predictions, useful for model interpretation and feature selection.\n",
        "\n",
        "### 13. Why is CatBoost efficient for categorical data?\n",
        "CatBoost excels with categoricals because:\n",
        "1. **Ordered Target Encoding:** Uses time-based scheme to prevent target leakage\n",
        "2. **One-Hot Encoding:** Automatically applied for small cardinality\n",
        "3. **Combination Features:** Creates interactions between categoricals\n",
        "4. **Ordered Boosting:** Special mode for categoricals\n",
        "5. **Native Handling:** No need for manual preprocessing\n",
        "\n",
        "This makes it robust against overfitting on categorical variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LA9ICC3FpKIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 14. AdaBoost Classifier with Accuracy\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost\n",
        "ada = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = ada.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "```\n",
        "\n",
        "### 15. AdaBoost Regressor with MAE\n",
        "```python\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create sample dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost\n",
        "ada_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "ada_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = ada_reg.predict(X_test)\n",
        "print(f\"MAE: {mean_absolute_error(y_test, y_pred):.4f}\")\n",
        "```\n",
        "\n",
        "### 16. Gradient Boosting Classifier with Feature Importance\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train GBM\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Feature importance\n",
        "for name, importance in zip(data.feature_names, gbc.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")\n",
        "```\n",
        "\n",
        "### 17. Gradient Boosting Regressor with R² Score\n",
        "```python\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create sample data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train GBM\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(f\"R² Score: {r2_score(y_test, y_pred):.4f}\")\n",
        "```\n",
        "\n",
        "### 18. XGBoost vs Gradient Boosting Accuracy Comparison\n",
        "```python\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Create data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train models\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "xgb.fit(X_train, y_train)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Compare\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, xgb.predict(X_test)):.4f}\")\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, gbc.predict(X_test)):.4f}\")\n",
        "```\n",
        "\n",
        "### 19. CatBoost Classifier with F1-Score\n",
        "```python\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost\n",
        "cat = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
        "cat.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = cat.predict(X_test)\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
        "```\n",
        "\n",
        "### 20. XGBoost Regressor with MSE\n",
        "```python\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "print(f\"MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n",
        "```\n",
        "\n",
        "### 21. AdaBoost Classifier with Feature Importance Visualization\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Using breast cancer dataset for better feature names\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train AdaBoost with decision stumps\n",
        "ada = AdaBoostClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(max_depth=1),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "ada.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(data.feature_names, ada.feature_importances_)\n",
        "plt.title(\"AdaBoost Feature Importance\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 22. Gradient Boosting Regressor Learning Curves\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Create data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "\n",
        "# Get learning curve data\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    X, y, cv=5, scoring='neg_mean_squared_error',\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
        ")\n",
        "\n",
        "# Calculate mean and std\n",
        "train_mean = -train_scores.mean(axis=1)\n",
        "test_mean = -test_scores.mean(axis=1)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_mean, label='Training error')\n",
        "plt.plot(train_sizes, test_mean, label='Validation error')\n",
        "plt.ylabel('MSE')\n",
        "plt.xlabel('Training set size')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 23. XGBoost Classifier Feature Importance\n",
        "```python\n",
        "from xgboost import plot_importance\n",
        "\n",
        "# Using breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost\n",
        "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plot_importance(xgb)\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 24. CatBoost Classifier Confusion Matrix\n",
        "```python\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Create binary classification data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train CatBoost\n",
        "cat = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
        "cat.fit(X_train, y_train)\n",
        "\n",
        "# Plot confusion matrix\n",
        "cm = confusion_matrix(y_test, cat.predict(X_test))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 25. AdaBoost with Different Estimators\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "# Create data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different n_estimators\n",
        "n_estimators_list = [10, 50, 100, 200, 500]\n",
        "accuracies = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    ada = AdaBoostClassifier(n_estimators=n, random_state=42)\n",
        "    ada.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, ada.predict(X_test))\n",
        "    accuracies.append(acc)\n",
        "    print(f\"n_estimators: {n}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(n_estimators_list, accuracies, marker='o')\n",
        "plt.xlabel('Number of Estimators')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Performance vs Number of Estimators')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 26. Gradient Boosting Classifier ROC Curve\n",
        "```python\n",
        "from sklearn.metrics import RocCurveDisplay\n",
        "\n",
        "# Using breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train GBM\n",
        "gbc = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gbc.fit(X_train, y_train)\n",
        "\n",
        "# Plot ROC\n",
        "RocCurveDisplay.from_estimator(gbc, X_test, y_test)\n",
        "plt.title('ROC Curve')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 27. XGBoost Regressor with Learning Rate Tuning\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create data\n",
        "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': [50, 100, 200]\n",
        "}\n",
        "\n",
        "# Grid search\n",
        "xgb_reg = XGBRegressor(random_state=42)\n",
        "grid = GridSearchCV(xgb_reg, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Results\n",
        "print(f\"Best learning rate: {grid.best_params_['learning_rate']}\")\n",
        "print(f\"Best n_estimators: {grid.best_params_['n_estimators']}\")\n",
        "print(f\"Best MSE: {-grid.best_score_:.4f}\")\n",
        "```\n",
        "\n",
        "### 28. CatBoost on Imbalanced Data with Class Weighting\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Create imbalanced data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Method 1: Class weights\n",
        "cat_weighted = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    class_weights=[1, 5],  # Higher weight for minority class\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "cat_weighted.fit(X_train, y_train)\n",
        "\n",
        "# Method 2: SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_res, y_res = smote.fit_resample(X_train, y_train)\n",
        "cat_smote = CatBoostClassifier(iterations=100, verbose=0, random_state=42)\n",
        "cat_smote.fit(X_res, y_res)\n",
        "\n",
        "# Compare\n",
        "print(\"With Class Weighting:\")\n",
        "print(classification_report(y_test, cat_weighted.predict(X_test)))\n",
        "print(\"\\nWith SMOTE:\")\n",
        "print(classification_report(y_test, cat_smote.predict(X_test)))\n",
        "```\n",
        "\n",
        "### 29. AdaBoost with Different Learning Rates\n",
        "```python\n",
        "# Create data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Test different learning rates\n",
        "learning_rates = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
        "accuracies = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    ada = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n",
        "    ada.fit(X_train, y_train)\n",
        "    acc = accuracy_score(y_test, ada.predict(X_test))\n",
        "    accuracies.append(acc)\n",
        "    print(f\"Learning Rate: {lr}, Accuracy: {acc:.4f}\")\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(learning_rates, accuracies, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Learning Rate (log scale)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('AdaBoost Performance vs Learning Rate')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "### 30. XGBoost for Multi-class Classification with Log Loss\n",
        "```python\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Create multi-class data\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, n_informative=4, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train XGBoost\n",
        "xgb = XGBClassifier(n_estimators=100, objective='multi:softprob', random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_probs = xgb.predict_proba(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(f\"Log Loss: {log_loss(y_test, y_probs):.4f}\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "jNR-pIkZrMDX"
      }
    }
  ]
}